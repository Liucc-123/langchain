{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1、Memory模块的设计思路\n",
    "1. 层次1(最直接的方式)：保留一个聊天消息列表\n",
    "2. 层次2(简单的新思路)：只返回最近交互的k条消息\n",
    "3. 层次3(稍微复杂一点)：返回过去k条消息的简洁摘要\n",
    "4. 层次4(更复杂)：从存储的消息中提取实体，并且仅返回有关当前运行中引用的实体的信息"
   ],
   "id": "9a9678a7b7847a62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2、ChatMessageHistory（最底层）\n",
    "`ChatMessageHistory` 是一个用于 存储和管理对话消息 的基础类，它直接操作消息对象（如HumanMessage, AIMessage 等），是其它记忆组件的底层存储工具。\n",
    "在API文档中，ChatMessageHistory 还有一个别名类：InMemoryChatMessageHistory；导包时，需使用 `from langchain.memory import ChatMessageHistory`\n",
    "特点：\n",
    "* 仅仅只做消息的存储，对消息不做任何其他处理（如生成摘要、缓冲、窗口等等）\n",
    "* 不涉及消息的格式化，比如将消息转换为文本字符串"
   ],
   "id": "c33f20aabce5d6d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 场景1：`ChatMessageHistory`组件的使用",
   "id": "72e02d2864312c78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T13:03:04.503919Z",
     "start_time": "2025-10-23T13:03:04.081489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#1.导入相关包\n",
    "from langchain_classic.memory import ChatMessageHistory\n",
    "\n",
    "#2.实例化ChatMessageHistory对象\n",
    "history = ChatMessageHistory()\n",
    "# 3.添加UserMessage\n",
    "history.add_user_message(\"hi!\")\n",
    "# 4.添加AIMessage\n",
    "history.add_ai_message(\"whats up?\")\n",
    "# 5.返回存储的所有消息列表\n",
    "print(history.messages)"
   ],
   "id": "70c739ea6c7d5acd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='hi!', additional_kwargs={}, response_metadata={}), AIMessage(content='whats up?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 场景2：对接LLM",
   "id": "9da2623a65e29666"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T13:29:30.418017Z",
     "start_time": "2025-10-23T13:29:18.151665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from langchain_classic.memory import ChatMessageHistory\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_ai_message(\"我是一个无所不能的小智\")\n",
    "history.add_user_message(\"你好，我叫小明，请介绍一下你自己\")\n",
    "history.add_user_message(\"我是谁呢？\")\n",
    "# 创建大模型实例\n",
    "llm = ChatOllama(model='deepseek-r1:7b', base_url=\"http://localhost:11434\")  # 使用本地大模型\n",
    "response = llm.invoke(history.messages)\n",
    "print(response.content)"
   ],
   "id": "256b84468881c868",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我是小智，一个由深度求索（DeepSeek）公司开发的智能助手。我是一个通用人工智能工具，能够理解和回答各种问题、提供信息、进行对话交流，并帮助用户完成多种任务。\n",
      "\n",
      "作为一个AI，我的主要功能包括：\n",
      "1. **信息检索**：我可以帮助你查找资料、解答问题。\n",
      "2. **学习与推理**：虽然我没有意识和自我，但我可以通过大数据分析和算法推理来帮助你解决问题。\n",
      "3. **语言支持**：我能用中文进行自然的交流，并理解并回应你的指令。\n",
      "\n",
      "我的目标是为你提供便捷的帮助，无论是学习、工作还是生活中的问题，我都会尽力提供准确、有用的信息或解决方案。如果你有任何问题或需要帮助的地方，请随时告诉我！😊\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ConversationBufferMemory\n",
    "`ConversationBufferMemory` 是一个基础的 对话记忆（Memory）组件 ，专门用于按原始顺序存储完整的对话历史。\n",
    "适用场景：对话轮次较少、依赖**完整上下文**的场景（如简单的聊天机器）\n",
    "特点：\n",
    "* 存储完整的对话历史，不进行任何处理\n",
    "* 与 Chains/Models 无缝集成\n",
    "* 支持两种返回格式（通过 `return_messages` 参数控制输出格式，默认为False），return_messages=True时返回消息对象列表；return_messages=False时返回纯文本字符串"
   ],
   "id": "829204e79207b149"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 场景1：入门使用\n",
    "举例1："
   ],
   "id": "e27b44f461baf371"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T13:39:16.179962Z",
     "start_time": "2025-10-23T13:39:16.176450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.导入相关包\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "# 2.实例化ConversationBufferMemory对象\n",
    "memory = ConversationBufferMemory()\n",
    "# 3.保存消息到内存中\n",
    "memory.save_context(inputs={\"input\": \"你好，我是人类\"}, outputs={\"output\": \"你好，我是AI助手\"})\n",
    "memory.save_context(inputs={\"input\": \"很开心认识你\"},\n",
    "                    outputs={\"output\": \"我也是\"})\n",
    "# 4.读取内存中消息（返回消息内容的纯文本）\n",
    "print(memory.load_memory_variables({}))"
   ],
   "id": "a2f6222b37d4cd15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: 你好，我是人类hh\\nAI: 你好，我是AI助手\\nHuman: 很开心认识你\\nAI: 我也是'}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> !说明\n",
    "> * `save_context()`方法的`inputs`参数对应地就是human消息，`outputs`参数对应地就是ai消息。因此，inputs和outputs参数对应字典值的key不一定就需要时input or output，可以是任何其他值的key，比如\"input1\"都是ok的\n",
    "> * `load_memory_variables()`方法返回的是一个字典，key默认是“history”，这个后面在后面有大用。"
   ],
   "id": "ae051af6efa97209"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例2：",
   "id": "697ae546fe2fc89f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T13:42:05.136479Z",
     "start_time": "2025-10-23T13:42:05.132909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.导入相关包\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "# 2.实例化ConversationBufferMemory对象\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "# 3.保存消息到内存中\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "# 4.读取内存中消息（返回消息）\n",
    "print(memory.load_memory_variables({}))\n",
    "# 5.读取内存中消息( 访问原始消息列表)\n",
    "print(memory.chat_memory.messages)"
   ],
   "id": "8ddc931691bafebb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content='whats up', additional_kwargs={}, response_metadata={})]}\n",
      "[HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content='whats up', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 场景2：结合chain\n",
    "举例1：使用PromptTemplate和默认的memory_key"
   ],
   "id": "3882e3dc28f95617"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T13:48:29.754073Z",
     "start_time": "2025-10-23T13:48:20.706815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_classic.chains.llm import LLMChain\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 初始化大模型\n",
    "llm = ChatOllama(model='deepseek-r1:7b', base_url=\"http://localhost:11434\")\n",
    "# 创建提示\n",
    "# 有两个输入键：实际输入与来自记忆类的输入 需确保PromptTemplate和ConversationBufferMemory中的键匹配\n",
    "template = \"\"\"你可以与人类对话。\n",
    "当前对话历史: {history}\n",
    "人类问题: {question}\n",
    "回复:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "# 创建ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "# 初始化链\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "# 提问\n",
    "res1 = chain.invoke({\"question\": \"我的名字叫Tom\"})\n",
    "print(res1)\n",
    "res1 = chain.invoke({\"question\": \"你还记得我的名字吗\"})\n",
    "print(res1)"
   ],
   "id": "a50ad6bd5fa8ce9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '我的名字叫Tom', 'history': '', 'text': '你好！很高兴见到你，Tom！如果你有任何问题或需要帮助的地方，请随时告诉我。我在这里为你提供帮助。😊'}\n",
      "{'question': '你还记得我的名字吗', 'history': 'Human: 我的名字叫Tom\\nAI: 你好！很高兴见到你，Tom！如果你有任何问题或需要帮助的地方，请随时告诉我。我在这里为你提供帮助。😊', 'text': '当然记得！我是AI助手，随时为你提供帮助。😊'}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例2：可以通过memory_key修改memory数据的变量名",
   "id": "9cc3dea5f06d1385"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T13:50:38.191410Z",
     "start_time": "2025-10-23T13:50:28.512093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_classic.chains.llm import LLMChain\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 初始化大模型\n",
    "llm = ChatOllama(model='deepseek-r1:7b', base_url=\"http://localhost:11434\")\n",
    "# 创建提示\n",
    "# 有两个输入键：实际输入与来自记忆类的输入 需确保PromptTemplate和ConversationBufferMemory中的键匹配\n",
    "template = \"\"\"你可以与人类对话。\n",
    "当前对话历史: {chat_his}\n",
    "人类问题: {question}\n",
    "回复:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "# 创建ConversationBufferMemory(修改默认的memory key)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_his\")\n",
    "# 初始化链\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "# 提问\n",
    "res1 = chain.invoke({\"question\": \"我的名字叫Tom\"})\n",
    "print(res1)\n",
    "res1 = chain.invoke({\"question\": \"我的名字是什么？\"})\n",
    "print(res1)"
   ],
   "id": "83feb97596d0d7b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '我的名字叫Tom', 'chat_his': '', 'text': '你好，Tom！很高兴认识你。有什么我可以帮你的吗？'}\n",
      "{'question': '我的名字是什么？', 'chat_his': 'Human: 我的名字叫Tom\\nAI: 你好，Tom！很高兴认识你。有什么我可以帮你的吗？', 'text': '很高兴认识你，Tom！你的名字是Tom。有什么我可以帮你的吗？'}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例3：使用ChatPromptTemplate 和 return_messages",
   "id": "99b7706ea491b93f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T13:56:20.147674Z",
     "start_time": "2025-10-23T13:56:08.747972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.导入相关包\n",
    "from langchain_classic.chains.llm import LLMChain\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import MessagesPlaceholder,ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "# 2.创建LLM\n",
    "llm = ChatOllama(model='deepseek-r1:7b', base_url=\"http://localhost:11434\")\n",
    "# 3.创建Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "(\"system\",\"你是一个与人类对话的机器人。\"),\n",
    "MessagesPlaceholder(variable_name='history'),\n",
    "(\"human\",\"问题：{question}\")\n",
    "])\n",
    "# 4.创建Memory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "# 5.创建LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt,llm=llm, memory=memory)\n",
    "# 6.调用LLMChain\n",
    "res1 = llm_chain.invoke({\"question\": \"中国首都在哪里？\"})\n",
    "print(res1,end=\"\\n\\n\")\n",
    "res2 = llm_chain.invoke({\"question\": \"我刚刚问了什么\"})\n",
    "print(res2)"
   ],
   "id": "6be97e92e26df401",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '中国首都在哪里？', 'history': [HumanMessage(content='中国首都在哪里？', additional_kwargs={}, response_metadata={}), AIMessage(content='中国首都北京位于中国东部，具体位于 deletes the Tibet region, Gansu province, and Shanghai市。北京不仅是中国的政治、经济和文化中心，也是世界上最大的国际大都市之一。', additional_kwargs={}, response_metadata={})], 'text': '中国首都北京位于中国东部，具体位于 deletes the Tibet region, Gansu province, and Shanghai市。北京不仅是中国的政治、经济和文化中心，也是世界上最大的国际大都市之一。'}\n",
      "\n",
      "{'question': '我刚刚问了什么', 'history': [HumanMessage(content='中国首都在哪里？', additional_kwargs={}, response_metadata={}), AIMessage(content='中国首都北京位于中国东部，具体位于 deletes the Tibet region, Gansu province, and Shanghai市。北京不仅是中国的政治、经济和文化中心，也是世界上最大的国际大都市之一。', additional_kwargs={}, response_metadata={}), HumanMessage(content='我刚刚问了什么', additional_kwargs={}, response_metadata={}), AIMessage(content='您上一次问了：“问题：我刚刚问了什么”。', additional_kwargs={}, response_metadata={})], 'text': '您上一次问了：“问题：我刚刚问了什么”。'}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "总结：通过上面的例子，我们可以得到，对于memory，PromptTemplate 和 ChatPromptTemplate有以下几种区别：\n",
    "|特性|PromptTemplate|ChatPromptTemplate|\n",
    "|----|----|----|\n",
    "|消息存储时机|调用完成后|调用前存储用户输入，调用后存储模型输出|\n",
    "|消息调用显示|第一次调用history变量为空|第一次history变量就已经存储完整对话信息|\n",
    "|消息类型|字符串|List[BaseMessage]|\n",
    "\n",
    "注意：\n",
    "我们观察到的现象不是 bug，而是 LangChain 为 保障对话一致性 所做的刻意设计：\n",
    "1. 用户提问后，系统应立即\"记住\"该问题\n",
    "2. AI回答后，该响应应即刻加入对话上下文\n",
    "3. 返回给客户端的结果应反映最新状态"
   ],
   "id": "db3df69466cb4cf5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4、ConversationChain\n",
    "`ConversationChain`实质上就是封装了`LLMChain`和`ConversationBufferMemory`的Chain，提供了默认的PromptTemplate和Memory，你也可以不用。\n"
   ],
   "id": "5ec1d408a99de4d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例1：使用自定义的PromptTemplate，省略Memory",
   "id": "f27f6bae804858b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T14:24:21.799262Z",
     "start_time": "2025-10-23T14:24:08.992469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_classic.chains.conversation.base import ConversationChain\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 初始化大模型\n",
    "llm = ChatOllama(model='deepseek-r1:7b', base_url=\"http://localhost:11434\")\n",
    "# 创建提示\n",
    "template = \"\"\"你可以与人类对话。\n",
    "当前对话历史: {history}\n",
    "人类问题: {input}\n",
    "回复:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "# 初始化链\n",
    "chain = ConversationChain(llm=llm, prompt=prompt)\n",
    "# 提问\n",
    "res1 = chain.invoke({\"input\": \"我的名字叫Tom\"})\n",
    "print(res1)\n",
    "res1 = chain.invoke({\"input\": \"我的名字是什么？\"})\n",
    "print(res1)"
   ],
   "id": "4d9f9c5bceb21e95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '我的名字叫Tom', 'history': '', 'response': '你好！我是R，很高兴认识你。有什么我可以帮助你的吗？'}\n",
      "{'input': '我的名字是什么？', 'history': 'Human: 我的名字叫Tom\\nAI: 你好！我是R，很高兴认识你。有什么我可以帮助你的吗？', 'response': '你好！我叫Tom。很高兴认识你！有什么我可以帮助你的吗？'}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例2：使用内置的提示词模板和Memory",
   "id": "836d045cb4a2c3ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T14:30:53.993955Z",
     "start_time": "2025-10-23T14:30:35.650688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_classic.chains.conversation.base import ConversationChain\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 初始化大模型\n",
    "llm = ChatOllama(model='deepseek-r1:7b', base_url=\"http://localhost:11434\")\n",
    "# 初始化链\n",
    "chain = ConversationChain(llm=llm)\n",
    "# 提问\n",
    "res1 = chain.invoke({\"input\": \"我的名字叫Tom\"})\n",
    "print(res1)\n",
    "res1 = chain.invoke({\"input\": \"请问我叫什么\"})\n",
    "print(res1)"
   ],
   "id": "eaea2a20cb6389a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '我的名字叫Tom', 'history': '', 'response': 'Based on the conversation between Tom and the AI, here is an organized summary of the thought process:\\n\\n1. **Understanding the Context**: The user provided their name as \"Tom\" in English, prompting the AI for additional details about his full name.\\n\\n2. **Common Names Consideration**: Since Tom is a common first name, the AI considered possible surnames and middle names associated with it.\\n\\n3. **Possibilities Explored**:\\n   - Tom could be a full name.\\n   - Tom might have a middle name or an Asian surname.\\n   - There\\'s also the possibility of changing his name to another.\\n\\n4. **Conclusion**: Without further information, the best approach is to request more details from Tom to provide a precise response.\\n\\nThis structured thought process leads to the conclusion that additional information is needed before providing a specific answer.'}\n",
      "{'input': '请问我叫什么', 'history': 'Human: 我的名字叫Tom\\nAI: Based on the conversation between Tom and the AI, here is an organized summary of the thought process:\\n\\n1. **Understanding the Context**: The user provided their name as \"Tom\" in English, prompting the AI for additional details about his full name.\\n\\n2. **Common Names Consideration**: Since Tom is a common first name, the AI considered possible surnames and middle names associated with it.\\n\\n3. **Possibilities Explored**:\\n   - Tom could be a full name.\\n   - Tom might have a middle name or an Asian surname.\\n   - There\\'s also the possibility of changing his name to another.\\n\\n4. **Conclusion**: Without further information, the best approach is to request more details from Tom to provide a precise response.\\n\\nThis structured thought process leads to the conclusion that additional information is needed before providing a specific answer.', 'response': '请提供更多的信息以帮助我更好地理解您的名字。'}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5、ConversationBufferWindowMemory\n",
   "id": "c098218941593644"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
